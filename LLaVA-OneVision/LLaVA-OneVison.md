![image](LLaVA-OneVision/image/1.png)

论文地址：<https://arxiv.org/pdf/2408.03326>

项目地址：<https://github.com/LLaVA-VL/LLaVA-NeXT>

### 研究动机 
- 在有限计算成本下，基于LLaVA-Next的工作，探索一种可扩展、开放、统一的多模态架构，使模型能够通过简单的数据与结构扩展，从单图像理解自然迁移到多图、视频、3D等复杂视觉任务，从而推动通用视觉语言助手的实现。
- 目标：
	- **最大化复用已有的强大预训练模型能力**（LLM 与视觉模型）
	- **最小化多模态适配层的复杂度与计算开销**
	- **确保架构具备良好的扩展性**，能随数据量或模型规模提升而稳步增强性能
	![image](LLaVA-OneVision/image/19.png)


### A.模型架构
![image](LLaVA-OneVision/image/2.png)

**Vision Encoder:** SigLIP
**Projection:** 2-Layer MLP
**Language Model:** Qwen-2

输入图像 $X_V$ ，使用视觉编码器提取图像特征，即 $Z_v = g(X_v)$ ，随后将提取到的图像特征 $Z_v$ 送入投影层 $W$ ，经过 **2-Layer MLP** 得到 $H_v$ ，即 $$H_V=p(Z_v), Z_v = g(X_v)$$
对于长度为 $L$ 的序列，通过如下的方式计算目标答案 $X_a$ 的概率 
![image](LLaVA-OneVision/image/3.png)
可以从该计算方式看出来，LLaVA-OneVision统一了单图、多图与视频帧。 只需把不同的视觉信号当作 **图像序列** 输入 vision encoder 即可，无需额外改动，提高了通用性，具有强大的泛化能力。

同时，实验提及了“The grid features before and after the last Transformer layer are considered in our experiments.”说明论文在有意识的比较最后一层Transformer前后的特征有什么不同。
结果显示，**最后一层Transformer前**的特征（含更多空间结构细节）**更适合多图以及视频任务**，而**最后一层Transformer后**的特征（全局语义清晰）**更适合单图任务**。

##### 各组件选择的原因：
- **LLM — Qwen-2**
	- **强大的语言理解与推理能力**
	- **多尺寸可选**
	- **迁移性强**
	- LLaVA系列的经验表明：**语言模型越强，多模态能力越强**。
 - **Vision Encoder — SigLIP**
	 - **开源视觉编码器中性能最优**
	 - **视觉特征更具泛化性**
	 - **兼容 AnyRes设计**
	 - **实验验证表明SigLIP用在LLM上更优**
- **Projector — 两层 MLP**
	- 沿用已经被证实有效的LLaVA系列设计
	- **结构简单，训练稳定，效果不错**


### B.视觉表征

视觉输入表征由两个变量决定：
1. **分辨率（resolution）**：决定每个 patch 包含多少像素细节；
2. **token 数量（#tokens）**：决定视觉特征的表达长度。

提升前者，能显著提升细节任务表现且成本中等；提升后者，提升有限(会有冗余)，而且会显著增大LLM的输入序列，造成成本的直线上手。所以，若想**提高模型的细节任务表现**，应当平衡好分辨率与token数量，**尽量提升分辨率**且**保持一定的token数量**(既不过高而使计算成本上升，也不因为过低使表达能力)。所以，**在性能与成本的平衡上**，**提高分辨率**比**增加token数**更**划算**

基于此，他们推荐使用**AnyRes with Pooling**，并提出了 **Higher AnyRes**

#### AnyRes with Pooling
**(1)图像裁剪（Crops）**
	设定一个 **空间配置 (a, b)**：a= 水平方向分割数，b = 垂直方向分割数
	将原图划分成  a × b 个小图块（crop）
	每个 crop 都会被 **缩放到视觉编码器的标准输入大小**

**(2)token 数量**
	每个 crop 输入视觉编码器后，会被编码成 T 个视觉 token

**(3)总token数量**
					$L=(a×b+1)×T$
- a × b = crop 数量
- +1 = 除 crop 外，还保留**整体图像的特征**，提供全局上下文(全图下采样)
- T = 每个 crop 的 token 数

为了防止总token数过多带来的成本上升，设定token**阈值 τ**，并通过如下公式调整各图token数
![image](LLaVA-OneVision/image/4.png)
- **如果总 token 数超过阈值 τ** (L > τ)：
    - 将每个 crop 的 token 数**缩减到 $T_{new}$**
    - 保证最终总 token 数 ≈ **τ**
    - 缩减方式通常采用 **双线性插值，即把 patch embedding 或特征 map 缩小到新的 token 数，减少信息冗余，同时保持视觉结构。
- **如果总 token 数未超过 τ** (L ≤ τ)：
    - 保持原 token 数 T，不做裁剪
    - 保证信息完整性

##### 优势
- **统一表示格式**：无论是单图、多图、视频，视觉编码器输入都是 token 序列。
- **可调性**：可根据性能需求或计算预算，调整空间配置 (a, b) 与每 crop token 数量

#### Higher AnyRes 

基于AnyRes，论文将 AnyRes 改进，使其能够支持单图，多图，乃至视频帧的输入，扩展模型的任务类型，于是就提出了 Higher AnyRes。

#### **(1)token计算**
- 有 **$N$** 张图像（也可能是视频帧），对第 $i$ 张图定义空间配置 **$(a_i,b_i)$**
- 定义第 **$i$** 张图的 **crop 数**为 **$c_i=a_i×b_i$** 
- 每个 crop（以及每张图的 base image）在经过视觉编码器后产生 **$T_i$** 个 token。（注意：这里我们把 base image 的 token 数也记为 **$T_i$**，与 crop 相同的表示方式，方便统一计算）
- 因为保留 base image，所以第 $i$ 张图贡献的 token 数为 **$(c_i+1)×T_i$**
- 总 token 数：
![image](LLaVA-OneVision/image/5.png)
- 设定全局阈值为 **τ**。若 **L>τ**，则对每帧 / 每图按比例下采样 token
####  **(2)下采样**
如果 **L>τ**，则应用下采样策略，即![image](LLaVA-OneVision/image/6.png)
#### **(3)可控的 performance and cost**
- 如果想更快推理，那就降低 **τ**；如果想更细节，那就增大 **τ**；如果想快速处理视频，那就每帧减少 token 数。


##### 通过这样的方式，Higher AnyRes 实现了 **任意分辨率的输入**，**单图，多图，视频帧的统一流程处理**，以及**可控的performance and cost**


#### 处理方式及原因
##### **（1）Single-image**
**处理方式：**
- 选择大空间配置 (a,b)，保持原图高分辨率
- 给每张图分配较多 token，从而生成长 token 序列

**原因：**
- **保留更多细节，强化视觉基础能力**：单图问答、图像描述、视觉推理任务通常依赖高分辨率的细节（小物体、纹理、文字信息），这些都需要视觉基础能力扎实。而大空间配置 + 多 token 可以**捕获更多局部信息**，**显著提升模型基础视觉能力**。

- **训练数据丰富**：单图任务有大量高质量、多样化指令跟随数据，模型可以充分学习到丰富的视觉-语言对应关系，因此可以利用更多 token 提升表现。

- **模拟视频序列和多图**：长 token 序列与视频的帧序列以及多图序列形式类似，有助于 **single image→video 或 single image → multiple imgaes任务迁移**。也就是把**单图表示拉长成序列**，让 LLM 习惯处理序列化视觉信号，从而在处理视频帧和多图时更平滑，自然泛化到视频帧和多图任务。

##### **（2）Multi-image**
**处理方式：**
- 对每张图只使用 base image（不做多 crop）
- 每图的 token 数适中
- 将多图直接输入视觉编码器生成 feature map

**原因：**
- **计算开销**：多图同时处理，如果每张图都做多 crop 并保持高分辨率，**token 数会迅速膨胀**导致内存和计算量过高。

- **信息冗余少**：多图任务通常关注多张图之间的整体语义或比较（例如对比、关系推理），**不一定需要每张图都保留所有局部细节**。

- **效率优先**：保留 base image 即**可满足大多数多图任务需求**，同时允许模型同时处理更多图。

- **视觉基础强**：**单图提取信息的能力足够强**，能够在一定程度上减少因降低分辨率带来的影响

##### **（3）Video**
**处理方式：**
- 每帧统一缩放到 base image 分辨率
- 生成 feature map 后，如果 token 数过多，使用双线性插值下采样 token

**原因：**
- **帧数众多**：同多图，视频有大量帧，若每帧都使用高分辨率、多 crop，多 token，总 token 数会远超计算上限 **τ**，导致内存和计算量过高。

- **时间覆盖优先**：视频理解**更依赖时间维度**（帧间关系）而非单帧的极致空间细节。下采样 token 可以在**牺牲部分空间分辨率**的同时保留更多帧，从而更好捕获动作、场景变化和动态信息。

- **计算效率**：通过降低每帧 token 数，整体 token 数控制在 **τ** 以内，保证在有限显存和算力下可训练和推理。


总体如下图：
![image](LLaVA-OneVision/image/7.png)![image](LLaVA-OneVision/image/8.png)

### C.数据

#### 概要
- **“质量优于数量”**，**less is more**
- **数据多样性与平衡性**
- **持续更新与增量学习**
- **视觉指令微调**

#### (A) 高质量知识学习
###### **(1) Re-Captioned Detailed Description Data**
###### **(2) Document / OCR Data
###### (3) Chinese and Language Data

##### 特点与优势
- **合成数据占主导（99.8%）**：收集大规模真实高质量数据成本高且存在版权限制，但合成数据可以快速生成，易于扩展规模
- **自我增强**：用早期模型生成更丰富、更详细的数据，让ai自学习，提升细粒度能力
- **任务覆盖广**：覆盖图像描述、文档/文字理解、中文与多语言，从而提升多模态和多语言能力
- **规模可控**：合成数据易于扩展，能够满足大模型对数据量的需求

>“We believe that learning from large-scale synthetic data is becoming a trend as AI models >continue to grow more powerful.”
> 我们认为，随着 AI 模型越来越强大，从大规模合成数据中学习将成为一种趋势。

**LLaVA-OneVision 宣告了多模态模型从“人造数据”走向“模型自造数据”的时代。** 这是 AI 自我提升的关键趋势。


#### （B）Visual Instruction Tuning Data

论文从三个层次来划分Visual Instruction Tuning Data：
![image](LLaVA-OneVision/image/20.png)
###### i)Vision Input
1. **Single-image**
2. **Multi-image**
3. **Video**

覆盖三种数据，确保 LMM 能够**处理不同类型的视觉输入**，支持单图、图像集合和视频任务。

###### ii) Language Instruction
1. **General QA**：通用问答
2. **General OCR**：文本识别
3. **Doc/Chart/Screen**：文档、表格、屏幕信息理解
4. **Math Reasoning**：数学推理
5. **Language**：语言能力（多语言理解、生成等）

保证训练数据**在不同技能间分布均衡**，从而让模型**在多种任务上都能表现良好**。

###### iii) Language Response
1. **Free-form**：由先进模型生成，如 GPT-4V/o、Gemini，形式为自由文本，灵活回答、描述 。既可以保持多样化的语言风格，也能模拟真实人类交互，提高自然语言生成能力

2. **Fixed-form**： 人工审查学术数据集，检查并修改问题和答案格式。遵循 LLaVA-1.5 prompting strategy（即多选题，简答题，特定任务数据均覆盖）。使得**统一输出格式**，规范模型行为，**防止不同数据源冲突**，也**平衡 QA 性能、对话能力和复杂推理能力**。

明确模型回答格式，约束模型行为风格，对不同任务提供适合的输出形式

同时，论文还基于图像类型来划分指令数据，分为 **Single-Image Data** 和 **OneVision Data**。
前者是多模态能力的基础，后者是用于**在单图训练基础上进一步提升多模态能力**，从而让模型在 **单图，多图以及视频** 任务上表现均衡。

为了避免模型在某类任务或场景过拟合，论文将**平衡了多种单图任务类型的数据量**，同时**增大总数据量来提升基础视觉能力**；对于后者，添加**一定量的高质量单图数据**，**确保基础视觉能力不下降**，同时**混合多图以及视频帧数据**，提高任务迁移能力，总体上**平衡三者的数据分布**，保证平衡训练。

如下图：
![image](LLaVA-OneVision/image/9.png)
![image](LLaVA-OneVision/image/10.png)

### D.Training Strategies

为了让 LLM 具备多模态能力，论文选择了 **三类关键功能**：
- **单图指令跟随能力
    - 传统 LLaVA 主要关注的部分
    - 用于打基础，让模型学会视觉-语言对应关系
- **多图/视频任务能力
    - 更复杂的视觉输入
    - 在单图基础上迁移，提升模型处理多帧、多视角能力
- **高级任务与综合能力
    - 包含复杂推理、OCR、多语言理解等
    - 训练后期加入，更好地强化 emergent capabilities

并据此划分出三个训练阶段：
##### **Stage-1: Language-Image Alignment**
- **目标**：
    - 将视觉特征有效映射到 LLM 的词向量空间
    - 形成 **基础视觉-语言对齐能力**
- **作用**：
    - 打好多模态理解基础
    - 为后续视觉指令任务提供稳定的表示
    
##### **Stage-1.5: High-Quality Knowledge Learning**
- **目标**：
    - 将 **高质量知识数据**注入模型
    - 在保证计算效率的前提下，让 LMM 学习新的知识，为下一阶段提供知识基础
- **训练策略**：
    - 配置与 Stage-2 保持一致（如 batch size、token 数量、训练步骤等）
- **作用**：
    - 强化模型知识库
    - 保证训练连续性和能力迁移
    - 使模型在学习新知识时可以 **无缝整合已有能力**
    - 模型不仅学会对齐视觉与语言，还能拥有**丰富的知识储备**，提升回答准确性和推理能力
    
##### **Stage-2: Visual Instruction Tuning**
- **目标**：
    - 教模型解决 **多样化视觉任务**（多图、视频、多场景）
    - 训练模型生成 **符合预期的输出
- **训练策略**：
    - 将 instruction 数据分组
    - 按顺序训练每组数据，由易到难，由基础到复杂，保证技能覆盖和均衡
    - 即**课程学习（Curriculum Learning）**，先简单任务再复杂任务。
- **作用**：
    - 强化任务指令跟随能力
    - 让模型具备处理复杂视觉任务的能力
    - 模型具备**全景多模态能力**，能处理单图、多图、视频任务，并输出自然语言或结构化回答

而对于Stage-2，论文又进一步划分为两个阶段，分别是Single-Image Training，OneVision Training。前者让模型掌握 **单图任务的多样指令跟随能力**，为后续多图和视频任务迁移奠定基础。后者让模型学会在不同视觉场景下跟随指令完成任务，实现**跨场景迁移学习**

![image](LLaVA-OneVision/image/16.png)
![image](LLaVA-OneVision/image/17.png)
![image](LLaVA-OneVision/image/18.png)
##### 具体策略：
- **逐步训练**：随着训练阶段推进，最大图像分辨率和视觉 token 数量**逐步增加**，从 Stage-1的 729 tokens 增加到 Stage-2 的 10× Stage-1。并且，Stage-1 **仅使用基础的图像表征**，而1.5和 2 **使用 AnyRes 来提高图像表征能力**。这样循序渐进的训练流程，能让模型逐**步适应长序列输入**，让模型在后期阶段能处理高分辨率图像和更多 token 的多图/视频任务。同时还可以**在固定计算预算下**平衡性能与计算成本，

- **参数冻结**：Stage-1 仅训练 Projection层，其他层冻结；Stage-1.5 和 2 更新整个模型，即LLM + Vision Encoder + Projector。前者**快速对齐视觉-语言**，防止预训练参数破坏，后者更新整个模型来**学习多场景的多样指令跟随能力**。

- **学习率**：视觉编码器的学习率比 LLM **小 5 倍**。一方面**保护视觉编码器预训练参数**，防止过渡更新其参数，另一方面**让 LLM 更快适应新任务**，学习多场景的多样指令跟随能力。

#### **优势**
- 渐进式训练**先易后难**，**避免**一次性训练太复杂，**导致梯度不稳定或收敛困难**。且**Checkpoint 可复用**，每个阶段输出 checkpoint，可用于实验或微调，节省计算资源。

- **分阶段微调**保护预训练特征，**避免破坏视觉编码器已有知识**，同时完成**高效知识注入**，不影响基础对齐能力，同时**逐步适应复杂任务**

- 数据策略恰当。数据量大且高质量单图数据打好基础能力，注入高质量知识为后续阶段备好知识基础，再混合视频/多图训练，实现跨场景迁移。一方面**兼备知识和任务**，一方面**避免了视频/多图任务数据稀缺问题**（单图到视频/多图的自然迁移使得即使数据稀缺也可以在视频/多图任务上表现优异）

- **长序列训练策略**。通过逐步增加 token 数来让**模型逐步适应长序列**，同时让**性能逐步提升**


三阶段流程如下图：
![image](LLaVA-OneVision/image/11.png)
### E.Results
##### 概要
实验部分采用 **标准化评测框架 LMMs-Eval**，在 **0-shot、greedy decoding** 条件下，系统地测试了模型在多种模态上的表现。结果表明LLaVA-OneVision 在绝大多数基准上优于开源模型，特别是其最大模型（72B）在整体性能上介于 **GPT-4V** 与 **GPT-4o** 之间，而且训练策略证明了高效、可扩展且具通用性的优点，这使得未来改进更加容易，方便未来的研究。

#### E1.单图任务基准（Single-Image Benchmarks）
为了验证模型在真实单图任务中的表现，论文在多个维度上系统评测了LLaVA-OneVision，主要分为三类：**图表 / 文档理解**（Chart, Diagram, and Document Understanding）；**感知与跨领域推理**（Perception and Multi-discipline Reasoning）；**真实场景与多轮视觉对话**（Real-world Understanding and Visual Chat.）。

在 “**图表 / 文档理解**” 上，LLaVA-OneVision 超越 GPT-4V，接近 GPT-4o

在 “**感知与跨领域推理**” 上，模型显著优于 GPT-4V，部分结果甚至接近 GPT-4o，体现出框架在复杂视觉-语言推理方面的强大能力。

在 **“真实场景与多轮对话”** 上，LLaVA-OneVision 虽然与GPT-4V和GPT-4O相比，仍有改进的空间，但与同规模开源模型表现相当，并在 **MM-LiveBench** 上表现突出，说明其具备较强的 **现实场景理解与泛化能力**

结果如下图：

![image](LLaVA-OneVision/image/12.png)

#### E2.Multi-Image Benchmarks
##### 概要
在多图交错场景下系统评估，覆盖任务如找不同、图像编辑指令、视觉叙事、富文本 VQA、跨图 VQA、Raven 谜题等；并包含多视角/3D 任务。值得注意的是，这些属于**in-domain**的评测。并且同时在若干 **out-of-domain** 多图任务上评估

**结果表明：**
单图阶段训练的 LLaVA-OneVision (SI) 在所有多图基准上均优于现有多图 LMMs

经过 OneVision后，模型在若干多图/多视角任务上对 GPT-4V **实现了显著超越**，尤其是在多图推理、找不同与 3D 环境理解等复杂任务上提升明显，且 OneVision 阶段对**多视角/多帧**基准的提升尤为显著（这些基准在单图数据中缺失），说明该阶段对跨场景能力扩展至关重要。

结果如下图：
![image](LLaVA-OneVision/image/13.png)
![image](LLaVA-OneVision/image/14.png)
#### E3.Video Benchmarks
##### 概要
LLaVA-OneVision 在多个视频基准上表现良好 ，与以往开源模型相比通常更优或相当，且在若些复杂的视频理解基准（如 EgoSchema、VideoMME）上优势更明显。同时在 ActivityNet-QA、MLVU、VideoMME 等基准上，LLaVA-OneVision 能与 GPT-4V 竞争，但在需要大量复杂多轮视觉对话或更深层交互的场景仍有提升空间。

评估发现，即便使用开源 LLM，LLaVA-OneVision 在多数视频任务上能跑赢或持平于以前更大的开源模型，说明训练配方（AnyRes、分阶段训练、高质量合成数据等）是有效的。

- 在 EgoSchema、VideoMME 这类需要时序理解与跨片段推理的数据集上，OneVision 表现尤其好，说明其时间序列处理和跨帧迁移策略奏效，**在复杂长视频与推理任务上优势明显**。
    
- 在 PerceptionTest 上，模型从 0.5B 缩放到 7B 带来的提升很小（仅 ~0.5 分），而其他数据集常见 ≥5 分的增幅，这表明**感知类能力更多取决于视觉模块而不是 LLM 大小**。
    
- 对于像 EgoSchema 这种需要深入推理的数据，**更大的 LLM能显著提高性能**，说明语言/推理能力在复杂时序理解中十分关键。

- 在 ActivityNet-QA，很多问题可单帧回答（例如“球是什么颜色”），因此**仅用单图训练的 SI 模型**已经能**很好处理这类问题**，OneVision 相对提升有限。

结果如下图：
![image](LLaVA-OneVision/image/15.png)

### F.Emerging Capabilities with Task Transfer（仅做简要阐述，详细请看原文）

#### 概要
LLaVA-OneVision 在训练后出现了一系列**迁移/组合产生的新能力**，表明通过单图、多图与视频任务的混合训练，模型能将在单一场景学到的技能灵活迁移并组合以解决未显式训练过的复杂任务。

- **S1 — 图表与表格联合理解**（）（从单图 → 多图迁移）  
    能把单独学到的图表理解与表格/图解理解合并，完成联合推理任务。
    
- **S2 — GUI 理解并执行多模态代理任务**（单图 + 多图迁移）  
    能识别手机 GUI 截图并给出操作步骤（例如打开某应用），结合 OCR 与多图关系推理能力。
    
- **S3 — Set-of-marks（SoM）推理**（由单图任务组合产生）  
    首次在开源 LMM 中表现出良好的 SoM 推理能力，推测来源于视觉指认 + OCR 能力的组合。
    
- **S4 — 图像→视频编辑指令生成**（单图 + 视频迁移）  
    根据静图生成用于视频创作的详细脚本/提示，融合单图编辑与视频描述技能。
    
- **S5 — 视频间差异分析**（多图 + 视频迁移）  
    可比较两段视频在角色、动作、场景变化等方面的差异，把“找不同”从图片推广到视频序列。
    
- **S6 — 自动驾驶多摄像头视频理解**（单图/多图 → 多视角视频迁移）  
    在四路摄像头视频中分别描述各视角、并给出自车行动建议，集成多面板理解、时空推理与详细描述能力。
    
- **S7 — 复合子视频理解**（多图 → 视频迁移）  
    能理解竖屏视频中由若干子场景组成的布局与叙事，结合帧序列与上下文推理。
    
- **S8 — 视频中的视觉提示理解**（单图 → 视频迁移）  
    能识别视频中被高亮/标注的区域并执行 OCR，如读出运动员背号，尽管未在视频提示数据上训练。
    
- **S9 — 在视频中引用图像查询**（单图强训练支撑）  
    支持基于一帧/一张图的查询去回答关于整段视频的问题，显示出强的单图基础训练对该能力的必要性。

### G.总结
LLaVA-OneVision 是一个开放的大型多模态模型，通过整合 LLaVA-NeXT 的经验、采用分阶段训练、AnyRes/Higher AnyRes 的视觉表示策略、以及以高质量（主要为合成）数据为主的训练集，成功在单图、多图与视频场景上实现了统一且可扩展的能力。通过先在大规模单图指令数据上打底、再用混合的多图/视频数据精调，模型展现出强劲的跨场景迁移与若干新兴能力。实验在标准化评测下表明：在多数基准上 LLaVA-OneVision 达到或接近最先进水平（在若些任务上优于现有开源模型并可与商业模型ChatGPT-4V/4o相抗衡）。该工作不仅证明了“成本高效且分阶段”的训练配方的有效性，也为社区提供了可复用的开源资源与继续扩展的起点。

### 测试
1)简述 LLaVA-OneVision 的整体架构

2)SigLIP 被选作视觉 encoder 的原因？

3)设计一个 ablation 实验评估 SigLIP 提取 before/after last layer 特征对 LLM 下游任务影响。

4)AnyRes 设计动机是什么？做了什么？它解决了哪些问题？

5)Higher AnyRes 是什么？和 AnyRes 的区别？

6)token 下采样为什么用 bilinear interpolation 而不是丢弃 token？

7)为什么在 feature map 上用 bilinear interpolation 而不是在原图像上做？

8)多图与视频为什么通常不做多 crop？

9)训练阶段（Stage-1/1.5/2）分别在做什么？为什么要分阶段？

10)为什么在 Stage-1 只训练 projector，而后面才训练全模型？

11)为什么强调“质量重于数量”？具体怎么实现？

12)如何控制模型在合成数据上不会学到错误/偏见？

13)为什么视觉 encoder lr 比 LLM lr 小 5 倍？

14)单图长序列模拟视频表示的动机是什么？

15)如何在推理时动态决定 token 分配？如何选取视频关键帧？如何为关键帧分配更多注意？

16)在视频理解上，模型是否只是利用静帧信息？如何证明模型学习到时序关系？

17)论文中 token 预算 τ 是如何选择与调参的？有自动化方法吗？

18)合成数据占 99.8%，这是否会导致分布偏差？如何缓解？

19)在训练数据使用早期模型自标注时如何避免“自我强化偏见”？